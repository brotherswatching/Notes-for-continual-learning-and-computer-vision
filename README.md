# Notes-for-analytic-continual-learning-and-computer-vision
这篇仓库是对我对于修过的分析持续学习课程与计算机视觉课程的简要回顾与总结，水平有限，谬误之处敬请指正。

## 目录
1. [分析持续学习](#分析持续学习)
    1. [分析学习](#分析学习)
    2. [持续学习](#持续学习)
    3. [分析学习+持续学习](#分析学习+持续学习)
2. [计算机视觉](#计算机视觉)

## 分析持续学习
### 分析学习

1.what
* 通过**闭合形式的解**而非迭代优化的过程来得到网络参数，进行神经网络训练的方法
* 通过线性代数知识（最小二乘法，Moore-Penrose逆）将非线性的学习问题转换为线性，使训练能在一个epoch中完成。
* 这种方法带来的优势有：快训练速度，高解释性。因为避免了迭代，自然也就没有梯度消失与梯度爆炸的问题。在小样本数据库的领域内使用效果尤佳。

2.how

关键在于使用**伪逆**

推导部分

![](/analytic-learning/how-1.jpg "")

![](/analytic-learning/how-2.jpg "")

3.例子

CPNet: Correlation Projection Analytic Learning

ACnnL: Analytic convolutional neural network learning

4.递归分析学习

将一次进行的最小二乘法分成几块线性进行计算。这个性质不会改变权重，也被称为**权重的不变性**

示意图和推导如下

![](/analytic-learning/BRMP-1.jpg "")

![](/analytic-learning/BRMP-2.jpg "")

![](/analytic-learning/BRMP-3.jpg "")

![](/analytic-learning/BRMP-4.jpg "")

5.分析学习面临的挑战

* 只有一个损失函数-最小均方差（mse）
* 比起梯度迭代的方法，缺少灵活性
* 欠拟合的风险

现在的解决思路：利用预训练模型——冻结**backbone**,只训练头部，实验证明也可以产生好的效果

### 持续学习

1.what

也叫增量学习和终身学习，目的是让机器有持续获得知识的能力。

分为三种：

* 类增量学习（Class incremental learning，CIL）
* 任务增量学习(Task incremental learning, TIL)
* 模态增量学习(Domain incremental learning, DIL)

类增量为如今的主流研究内容。举例说明。本来判断是鸟还是狗，学习老虎的数据后，判断是鸟还是狗还是老虎，以此类推，逐渐增加。

任务增量和模态增量较为简单。任务增量为增加任务，能知道任务内容，比如第一个任务是判断鸟还是狗，第二个任务判断鱼还是猫。模态增量为对同一个任务增加不同模态的识别，比如增加动画的模态，黑白的模态等。

2.评估指标

* 最终准确率
* 平均准确率
* 遗忘率
* 回忆率
* 新学率

3.how

如果我们直接持续的进行学习,即学习完task 1后学习task 2，会出现什么？--**灾难性遗忘**

如果学习完task 1后，将task 1 和task 2都重新进行学习，那么显然又**代价高昂**

1. 正则方法：Regularization = loss + penalty term，通过惩罚项减少旧任务的重要权重偏移。
   1. 权重正则：惩罚项的构成是以阻止重要权重为目标。如何判断权重重要有不同方法，如EWC。
   2. 功能正则：让新输出类似于之前，使用蒸馏进行惩罚。如LwF
2. 重播法：保留一部分之前的数据来缓解遗忘。
   1. 代表重播。只保留其*代表*。
   2. 生成重播。用生成模型模仿之前的真实数据，而非直接使用真实数据。
   3. 特征重播。之前数据的浓缩信息。
3. 基于结构：为每个任务分配一个子网络。（或模型的不同部分来学不同任务，PackNet）
4. 基于优化：优化模型
   1. 让各任务的方向的互相干扰减少（投射梯度，使其互不重叠，Adam-NSCL）
   2. 获得不同任务的总体知识（Meta Learning，学习去学习，通过相似任务学习共用知识，再在下游任务做微小更新）
5. 杂合方法：结合上述的不同方法。
   

4.挑战

* 遗忘：反向传播与梯度下降。在现在的任务中，旧任务的数据是*不可见*的，所以权重的计算只与新数据有关。
* 稳定性与可塑性的冲突：100%的可塑性意味着遗忘，100%的稳定性意味着无法学习新内容。二者不可得兼。
* 语义偏移：在潜空间里的旧任务的embedding在学习完新任务后会进行偏移
* 倾向近况：模型的预测总会倾向新任务，种类越旧，遗忘越强。
* 隐私问题：如果保留之前任务的数据的例子进行训练以缓解遗忘，则会造成隐私的泄露。

### 分析学习+持续学习

通过之前分析学习的铺垫，我们知道了分析学习的两大特性，**一个epoch完成训练**还有**权重不变性**。

能否将这两个特性用在CIL问题中呢？

我们知道，效果最好的当然是在获得新知识的时候把所有新旧所有数据都输入进去进行重新训练，但是代价太过高昂让我们不得不放弃。但是**权重不变性**正好解决了这点--分段迭代的训练和整体的训练最后的权重值是**一样的**。

![](/analytic-continual-learning/ACIL-1.jpg "")
H. Zhuang, et. al. , ACIL: Analytic Class-Incremental Learning with Absolute Memorization and Privacy Protection, NeurIPS, 2022.

分为两个部分
1. 基础训练部分，训练特征提取器和初始分析分类器
    1. 训练cnn基座（特征提取器）
    2. 用最小二乘法训练初始的分析分类器
![](/analytic-continual-learning/ACIL-2.jpg "")
2. 类增量学习部分，一阶段一阶段的训练分析分类器
    1. 固定基座，训练分类器
    2. 在自相关矩阵中储存信息
    3. 更新线性分类器的权重
![](/analytic-continual-learning/ACIL-3.jpg "")

数学推导如下：
![](/analytic-continual-learning/ACIL-4.jpg "")
![](/analytic-continual-learning/ACIL-5.jpg "")
![](/analytic-continual-learning/ACIL-6.jpg "")
   









## 计算机视觉
